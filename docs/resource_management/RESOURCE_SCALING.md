# Product scalingFor optimum performance and stability the appropriate resource `request` for each pod, and the number of pods in the product cluster should be carefully considered. Kubernetes provides a means for horizontally and vertically scaling the pods within a cluster, these approaches are described below.## Horizontal scalingThe Helm charts provision one `StatefulSet` by default. In order to horizontally scale up or down the cluster `kubectl scale` can be used at runtime to provision a multi-node Data Center cluster, with no further configuration required (although note that the Ingress must support cookie-based session affinity in order for the products to work correctly in a multi-node configuration). Here is the syntax for scaling up/down the Data Center cluster:```kubectl scale statefulsets <statefulsetset-name> --replicas=n```For details on the `cpu` and `memory` requirements see [Resource requests and limits](#Resource-requests-and-limits) ## Vertical scalingThe resource `requests` and `limits` for a `StatefulSet` can be defined either post product deployment or for deployments that are already running within the Kubernetes cluster. Take note that vertical scaling will result in the pod being re-created with the updated values### Prior to deploymentBefore performing a helm install update the appropriate products `values.yaml` `container` stanza with the desired `requests` and `limits` values i.e. ```yaml container:   limits:    cpu: "4"    memory: "4G"  requests:    cpu: "2"    memory: "2G"```### Post deploymentFor existing deployments the `requests` and `limits` values can be dynamically updated either declaratively or intrinsically #### DeclarativelyThis the preferred approach as it keeps the state of the cluster, and the helm charts themselves in sync.1. Update `values.yaml` appropriately2. Apply the patch:```shellhelm upgrade <release> <chart> -f <values file>```NOTE: This approach does not currently due to: [helm issue 7998](https://github.com/helm/helm/issues/7998).  #### IntrinsicallyUsing `kubectl edit` on the appropriate `StatefulSet` the respective `cpu` and `memory` values can be modified. Saving the changes will then result in the existing product pod(s) being re-provisioned with the updated values.## Resource requests and limitsTo ensure that Kubernetes appropriately schedules resources, the respective product `values.yaml` are configured with default `cpu` and `memory` [resource request values](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/).#### Resource requestsThe default resource request that are used for each product are defined below. Take note that these values are geared toward small data sets. For larger enterprise deployments refer to the data  enter infrastructure recommendations [here](https://confluence.atlassian.com/enterprise/data-center-infrastructure-recommendations-972333478.html) .  Using the [formula](#Memory-request-sizing), the `memory` specific values are derived from the default `JVM` requirements defined for each product's Docker container.| Product  | CPU   |  Memory ||----------|:-----:|------:|| [Jira](https://bitbucket.org/atlassian-docker/docker-atlassian-jira/src/master/#markdown-header-memory-heap-size)                    | `2`   | `2G`  || [Confluence](https://bitbucket.org/atlassian-docker/docker-atlassian-confluence-server/src/master/#markdown-header-memory-heap-size)   | `2`   | `2G`  || [Bitbucket](https://bitbucket.org/atlassian-docker/docker-atlassian-bitbucket-server/src/master/)                                    | `2`   | `2G`  || [Crowd](https://bitbucket.org/atlassian-docker/docker-atlassian-crowd/src/master/)                                                   | `2`   | `1G`  |#### Memory request sizingRequest sizing must allow for the size of the product `JVM`. That means the `maximum heap size`, `minumum heap size` and the `reserved code cache size` (if applicable) plus other JVM overheads, must be considered when defining the request `memory` size. As a rule of thumb the formula below can be used to deduce the appropriate request memory size.```shell(maxHeap + codeCache) * 1.5```#### Resource limitsEnvironmental and hardware constraints are different for each deployment, therefore the product `values.yaml` do not provide a resource [`limit`](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container) definition. Resource usage limits can be defined by updating the commented out `resources.container.limits` stanza within the appropriate product `values.yaml`.